[" If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter. Fields in structures might appear in order that is different from the order of declaration. Fields in structures might appear in order that is different from the order of declaration. The cuFFT library user guide. The user guide for CUB. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. The NPP library is written to maximize flexibility, while maintaining high performance. The user guide for the nvJitLink library. The user guide for the nvFatbin library. This facility can often provide optimizations and performance not possible in a purely offline static compilation. The CUPTI-API. The documentation for GPUDirect Storage. nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process. Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition getting started guide The documentation for Nsight Systems. The documentation for Nsight Visual Studio Edition. This is the guide to the Profiler. It also discusses EGL interoperability. All rights reserved.\r\n      Last updated on Jul 1, 2024.\r\n      ", "run) available in NVIDIA driver downloads. This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (\u201cNVIDIA\u201d) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (\u201cTerms of Sale\u201d). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer\u2019s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer\u2019s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer\u2019s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, \u201cMATERIALS\u201d) ARE BEING PROVIDED \u201cAS IS.\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA\u2019s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U. Other company and product names may be trademarks of the respective companies with which they are associated. \nPrivacy Policy\r\n|\r\nManage My Privacy\r\n|\r\nDo Not Sell or Share My Data\r\n|\r\nTerms of Service\r\n|\r\nAccessibility\r\n|\r\nCorporate Policies\r\n|\r\nProduct Security\r\n|\r\nContact\n \r\n  Copyright \u00a9 2018-2024, NVIDIA Corporation & affiliates", "  \n  \n       \n       \n       NVIDIA\u00ae Magnum IO GPUDirect\u00ae Storage (GDS) is part of the GPUDirect family. GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.  If this API is not called, an internal registered memory is used if required on the first time the buffer is used, for example, in cuFile{Read, Write}.  For a lustre filesystem, if greater than 0, compatibility mode is used for IO sizes between [1 - posix_gds_min_kb] specified in kB.   If this option is false, all writes to IBM Spectrum Scale will use compatibility mode.     These values cannot have any side effects on the file system, the application process, and the larger system. If explicit APIs are used, the incurred errors are reported immediately, but if the operations of these explicit APIs are performed implicitly, error reporting and handling are less clear. \n         \n  \n  \n        \n        Batch APIs are submitted synchronously, but executed asynchronously with respect to host thread. Completion of IO can be checked asynchronously using a status API in the same host thread or in a different thread. which describes the IO action, status, errors, and bytes transacted for each instance. \n           Parameters\n  Returns\n This can happen when the character device (/dev/nvidia_fs[0-15]) is restricted to certain users by an administrator, for example, admin, where /dev is not exposed with read permissions in the container. This API is used to set whether the library should use polling and the maximum IO threshold size less than or equal to which it will poll.poll_max_size_kb for the current process.  \n         \n          See the following for more information: \n           \n  \n CUfileError_t cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size); \n This parameter is used by the nvidia-fs driver as the maximum IO chunk size in which IO is issued to the underlying filesystem. In compatible mode, this is the maximum IO chunk size that the library uses to issue POSIX read/writes.   Parameters\n  max_direct_io_size\n  Returns\n  Description\n  This API is used with cuFileGetDriverProperties and is used to set the maximum direct IO size used by the library to specify the nvidia-fs kernel driver the maximum chunk size in which the latter can issue IO to the underlying filesystem. In compatible mode, this is the maximum IO chunk size which the library uses for issuing POSIX read/writes. This parameter is dependent on the underlying GPU hardware and system memory.max_direct_io_size_kb config key for the current process.   The device pointer addresses referred to in these APIs pertain to the current context for the caller. Application is allowed to open a file in non O_DIRECT mode in compat mode and also with nvidia-fs. In the latter case, an O_DIRECT path between GPU and Storage will be used if such a path exists. \n           Description\n\n This API should be invoked only after the application ensures there are no outstanding IO operations with the handle.This is a synchronous call and blocks until the IO is complete.   Returns\n  Description\n  This API writes the data from the GPU memory or the host memory to a file specified by the file handle at a specified offset and size bytes by using GDS functionality.This is a synchronous call and will block until the IO is complete. The data is not guaranteed to be present after a system crash unless the application uses an explicit fsync(2) call. \n           The API will synchronize on the stream before allocating resources. \n           The API will synchronize on the stream before releasing resources. If the exact size is not known at the time of I/O submission, then you must set it to the maximum possible I/O size for that stream I/O. If the exact size is not known at the time of I/O submission, then you must set it to the maximum possible I/O size for that stream I/O.io_batch_size\u201d \n               (Output) Will be used in subsequent batch IO calls.   Returns\n  Description\n  This interface should be the first call in the sequence of batch I/O operation.  Returns\n  Description\n Based on the type of memory pointer, the data is transferred to/from the GPU memory by using GDS or the data is transferred to/from the CPU memory. The minimum number of IO entries for which status is requested.  This is a pointer to max requested IO entries to poll for completion and is used as an Input/Output parameter. As an input *nr must be set to pass the maximum number of IO requests to poll for. As an output, *nr returns the number of completed I/Os.   This parameter is used to specify the amount of time to wait for in this API, even if the minimum number of requests have not completed. If the timeout hits, it is possible that the number of returned IOs can be less than min_nr. Individual IO status and error can be obtained by examining the returned status and error in the array iocbp. NVIDIA Corporation (\u201cNVIDIA\u201d) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\n   NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\n   NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (\u201cTerms of Sale\u201d). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\n   NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer\u2019s own risk.\n   NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. Weaknesses in customer\u2019s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\n   No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\n   Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\n   THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, \u201cMATERIALS\u201d) ARE BEING PROVIDED \u201cAS IS.\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA\u2019s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. \n         \n \n  NVIDIA, the NVIDIA logo, DGX, DGX-1, DGX-2, DGX-A100, Tesla, and Quadro are trademarks and/or registered trademarks of NVIDIA Corporation in the United States and other countries.  \n         \n Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright \u00a9 2024 NVIDIA Corporation", " This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA). The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains. The C++ parallel algorithms library. The user guide for Compute Sanitizer. The application notes for cuobjdump, nvdisasm, and nvprune. In this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively. Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms. The libNVVM API. The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels. NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR", " The APIs will refer to this memory address as buffer pointer unless the API specifically applies to a particular type of memory.  If true, forces compatibility mode to be used for writes where the file offset and/or IO size is not aligned to Page Boundary (4KB). \n ssize_t cuFileRead(CUFileHandle_t fh, void *bufPtr_base, size_t size, off_t file_offset, off_t devPtr_offset);\nssize_t cuFileWrite(CUFileHandle_t fh, const void *bufPtr_base, size_t size, off_t file_offset, off_t devPtr_offset); \n \n        \n         The starting offset of the buffer on the device or host is determined by a base (bufPtr_base) and offset (bufPtr_offset). This offset is distinct from the offset in the file. Developers must ensure that buffers are registered up front and off the critical path.   \n         \n          See the following for more information: \n           \n  \n ssize_t cuFileRead(CUfileHandle_tfh, void *bufPtr_base, size_t size, off_t file_offset, off_t bufPtr_offset); \n Parameters\n File descriptor for the file. Base address of buffer in device memory or host memory. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call.  Size in bytes to read. Offset in the file to read from. Offset relative to the bufPtr_base pointer to read into. This parameter should be used only with registered buffers.   Returns\n  Description\n  This API reads the data from a specified file handle at a specified offset and size bytes into the GPU memory by using GDS functionality or into the host memory based on the type of memory pointer. The API works correctly for unaligned offsets and any data size, although the performance might not match the performance of aligned reads.  \n          \n           For the bufPtr_offset, if data will be read starting exactly from the bufPtr_base that is registered with cuFileBufRegister, bufPtr_offset should be set to 0. To read starting from an offset in the registered buffer range, the relative offset should be specified in the bufPtr_offset, and the bufPtr_base must remain set to the base address that was used in the cuFileBufRegister call. \n           \n         \n          See the following for more information: \n           \n  \n ssize_t cuFileWrite(CUfileHandle_t fh, const void *bufPtr_base, size_t size, off_t file_offset, off_t bufPtr_offset); \n Parameters\n File descriptor for the file Base address of buffer in device memory or host memory. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call.  Size in bytes to which to write. Offset in the file to which to write. Offset relative to the bufPtr_base pointer from which to write. This parameter should be used only with registered buffers. The API works correctly for unaligned offset and data sizes, although the performance is not on-par with aligned writes. \n          \n  Refer to the note in cuFileRead for more information about bufPtr_offset:.  \n         \n          Refer to the following for more information: \n           \n \n \n  \n        \n        The device pointer addresses that are mentioned in the APIs in this section pertain to the current context for the caller. cuFileRead and cuFileWritemust use this bufPtr_base as the base address.  Size in bytes from the start of memory to map. \n           The stream pointer is expected to be a valid pointer. \n           The stream pointer is expected to be a valid pointer. Pointer to size in bytes to read.  Pointer to offset in the file from which to read.  Pointer to the offset relative to the bufPtr_base pointer from which to write.  Pointer to the bytes read from the specified filehandle. This pointer should be a non NULL value and *bytes_read_p set to 0.  The pointer to access that memory from the device can be obtained by using cuMemHostGetDevicePointer. The base address of the buffer in the memory from which to write. The buffer can be allocated using either cudaMemory/cudaMallocHost/malloc/mmap. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call.  Pointer to the size in bytes to write.  Pointer to the offset in the file from which to write.  Pointer to the offset relative to the bufPtr_base pointer from which to write.  Pointer to the bytes written to the specified filehandle.This pointer should be a non NULL value and *bytes_written_p set to 0.  The pointer contains the CUfileIOParams_t array structures of the length nrarray.h\"\nusing namespace std;\n\nint main(void) {\n        int fd;\n        ssize_t ret;\n        void *devPtr_base;\n        off_t file_offset = 0x2000;\n        off_t devPtr_offset = 0x1000;\n        ssize_t IO_size = 1UL << 24;\n        size_t buff_size = IO_size + 0x1000;\n        CUfileError_t status;\n        // CUResult cuda_result;\n        int cuda_result;\n        CUfileDescr_t cf_descr;\n        CUfileHandle_t cf_handle;\n        char *testfn;\n        \n        testfn=getenv(\"TESTFILE\");\n        if (testfn==NULL) {\n            std::cerr << \"No testfile defined via TESTFILE.\" << std::endl;\n\n        memset((void *)&cf_descr, 0, sizeof(CUfileDescr_t));\n        cf_descr", " \n                        \n                      \n                        For Cloud Service Providers (CSPs), who have multi-tenant use cases, MIG ensures one client \n                        cannot impact the work or scheduling of other clients, in addition to providing enhanced isolation for customers. MIG works with Linux operating systems, supports containers using Docker Engine, \n                        with support for Kubernetes and virtual machines using hypervisors such as Red Hat Virtualization and \n                        VMware vSphere.\n                        \n                      \n                        The purpose of this document is to introduce the concepts behind MIG, deployment considerations and \n                        provide examples of MIG management to demonstrate how users can run CUDA applications on MIG supported GPUs. \n                        \n                      MIG is supported on GPUs starting with the NVIDIA Ampere generation (i. The following table provides a list of supported GPUs: \n                      \n                        Additionally, MIG is supported on systems that include the supported products above such as \n                        DGX, DGX Station and HGX.\n                                 \n                                \n                           This section introduces some terminology used to describe the concepts behind MIG. A GPU SM slice is \n                              roughly one seventh of the total number of SMs available in the GPU when configured in MIG mode. For illustration purposes, the document will use the \n                           A100-40GB as an example, but the process is similar for other GPUs that support MIG.\n                                 \n                               \n                                 For a list of all supported combinations of profiles on MIG-enabled GPUs, refer to the section on \n                                 supported profiles.\n                              \n                            \n                           MIG has been designed to be largely transparent to CUDA applications - so that the \n                           CUDA programming model remains unchanged to minimize programming effort.  The following system considerations are relevant for when the GPU is in MIG mode.  MIG is supported only on Linux operating system distributions supported by CUDA.\n                               Also note the device nodes and nvidia-capabilities for exposing the MIG devices.06 and it is recommended to use the /dev based system-level interface \n                                    for controlling access mechanisms of MIG devices through cgroups.\n                                 \n                               Setting MIG mode on the A100/A30 requires a GPU reset (and thus super-user privileges). \n                                 Once the GPU is in MIG mode, instance management is then dynamic.\n                               On NVIDIA Ampere GPUs, similar to ECC mode, MIG mode setting is persistent across reboots until the user toggles the setting\n                                 explicitly\n                               All daemons holding handles on driver modules need to be stopped before MIG enablement.\n                               Toggling MIG mode requires the CAP_SYS_ADMIN capability. Other MIG management, such as creating and destroying instances, \n                                 requires superuser by default, but can be delegated to non-privileged users by adjusting permissions to MIG capabilities \n                                 in /proc/.\n                                 \n                                Users should note the following considerations when the A100 is in MIG mode: No graphics APIs are supported (e. using cuda-memcheck or compute-sanitizer) is supported\n                               CUDA MPS is supported on top of MIG. The table \n                        below highlights what the name of a MIG device would look like in this case.\n                           \n                         \n                           MIG supports running CUDA applications by specifying the CUDA device on which the application should be run. \n                           With CUDA 11/R450 and CUDA 12/R525, only enumeration of a single MIG instance is supported.\n                              \n                             \n                        This section provides an overview of the supported profiles and possible placements of the MIG profiles on \n                        supported GPUs.   The following prerequisites and minimum software versions are recommended when using supported GPUs in MIG mode.  MIG is supported only on GPUs and systems listed here It is recommended to install the latest NVIDIA datacenter driver. The minimum versions are provided below: Linux operating system distributions supported by \n                                 \n                                    CUDA If running containers or using Kubernetes, then: \n                           MIG can be managed programmatically using NVIDIA Management Library (NVML) APIs or its \n                           command-line-interface, nvidia-smi. \n                           \n                         \n                           For more information on the MIG commands, see the nvidia-smi man page or nvidia-smi mig --help. \n                           For information on the MIG management APIs, see the NVML header (nvml.h)\n                           For automated tooling support with configuring MIG, refer to the \n                           NVIDIA MIG Partition Editor (or mig-parted) \n                           tools. \n                           \n                         \n                           By default, MIG mode is not enabled on the GPU. For example, running nvidia-smi shows that MIG mode is disabled:\n                           \n                         \n                           MIG mode can be enabled on a per-GPU basis with the following command: \n                           nvidia-smi -i <GPU IDs> -mig 1. If no GPU ID is specified, then MIG mode is applied to all the GPUs on the system.\n                           \n                         \n                           When MIG is enabled on the GPU, depending on the GPU product, the driver will attempt to reset the GPU so that MIG mode can\n                           take effect. \n                           \n                         \n                              Starting with the Hopper generation of GPUs, enabling MIG mode no longer requires a GPU reset to take \n                              effect (and thus the driver does not attempt to reset the GPU in the background).\n                              \n                            \n                              Note that MIG mode (Disabled or Enabled states) is only \n                              persistent as long as the driver is resident in the system (i. MIG mode is \n                              no longer persistent across system reboots (there is no longer a status bit stored in the GPU InfoROM).  \n                              \n                            \n                              Thus, an unload and reload of the driver kernel modules will disable MIG mode.\n                              \n                            \n                              On NVIDIA Ampere GPUs, when MIG mode is enabled, the driver will attempt to reset the \n                              GPU so that MIG mode can take effect. \n                              \n                            \n                              Note that MIG mode (Disabled or Enabled states) \n                              is persistent across system reboots (there is a status bit stored in the GPU InfoROM). \n                              Thus MIG mode has to be explicitly disabled to return the GPU to its default state.\n                              \n                            \n                                 If you are using MIG inside a VM with NVIDIA Ampere GPUs (A100 or A30) in passthrough, then you may need to reboot the VM\n                                 to allow the GPU to be in MIG mode as in some cases, GPU reset is not \n                                 allowed via the hypervisor for security reasons. For example, on DGX systems, \n                              you may encounter the following message:\n                              \n                            \n                              In this specific DGX example, you would have to stop the nvsm and dcgm services, enable MIG mode on the desired GPU and then restore the monitoring services:\n                              \n                            \n                              The examples shown in the document use super-user privileges. As described in the  Device Nodes  \n                              section, granting read access to mig/config capabilities allows non-root users to manage instances once the GPU has been configured into MIG mode. \n                              The default file permissions on the mig/config file is shown below. \n                              \n                            \n                           The NVIDIA driver provides a number of profiles that users can opt-in for when configuring the MIG feature in A100. In \n                              other words, simply enabling MIG mode on the GPU is not sufficient. Also note that, the created MIG devices are not \n                              persistent across system reboots. Thus, the user or system administrator needs to recreate the desired MIG configurations\n                              \n                              if the GPU or system is reset. For automated tooling support for this purpose, refer to the \n                              NVIDIA MIG Partition Editor (or mig-parted) \n                              tool, including creating a \n                              systemd service that could recreate the MIG geometry at system startup.\n                           \n                         On Ampere GPUs (A100 or A30), NVML (and nvidia-smi) does not support attribution of utilization metrics to MIG devices. MPS and MIG can work together, potentially achieving even higher levels of utilization \n                           for certain workloads. \n                           \n                         \n                           In the following sections, we will walk through an example of running MPS on MIG devices.\n                           \n                         \n                                       Configure the desired MIG geometry on the GPU.\n                                       \n                                     \n                                       Launch the application by specifying the MIG device using CUDA_VISIBLE_DEVICES. However, this mode is not supported when the GPU is in MIG mode as we use multiple \n                                    MPS servers (one per MIG GPU instance).\n                                    \n                                  \n                              Follow the steps outlined in the previous sections to configure the desired MIG geometry on the GPU. This section provides an overview of running Docker containers on A100 with MIG. \n                              MIG support is available starting with v2. \n                              \n                            MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID> \n                                       when using R450 and R460 drivers or MIG-<UUID> starting with \n                                       R470 drivers.03, the --gpus option can be used to specify MIG devices by \n                              using the following format: \u2018\u201cdevice=MIG-device\u201d\u2019, where MIG-device can follow \n                              either of the format specified above for NVIDIA_VISIBLE_DEVICES. \n                              As can be seen in the example, only one MIG device as chosen is visible to the container when using either format. This is shown\n                              below:\n                              \n                            MIG support in Kubernetes is available starting with v0.\n                           Visit the documentation \n                           on getting started with MIG and Kubernetes.08, Slurm supports the usage of MIG devices.\n                        \n                      \n                        For example, the mig-config capability allows one to create and destroy MIG instances on any MIG-capable GPU \n                        (e. Without this capability, all attempts to create or destroy a MIG instance will fail.\n                           \n                         \n                           This can be seen in the example below:\n                           \n                         \n                           The combination of the device major for nvidia-caps and the value of DeviceFileMinor in this file indicate that the \n                           mig-config capability (which allows a user to create and destroy MIG devices) is controlled by the device node with a major:minor \n                           of 238:1. \n                           As such, one will need to use cgroups to grant a process read access to this device in order to configure MIG devices. +ur from a value of 256 (o400) from our example for mig-config).\n                           \n                         \n                           One final thing to note about /dev based capabilities is that the minor numbers for all possible capabilities are predetermined and can be \n                           queried under various files of the form:\n                           \n                         \n                           For example, all capabilities related to MIG can be looked up as:\n                           \n                         \n                           The format of the content follows: GPU<deviceMinor>/gi<GPU instance ID>/ci<compute instance ID> \n                              The NVML device numbering (e.\n                              \n                            \n                           For example, if the MIG geometry was created as below:\n                           \n                          Then the corresponding device nodes: /dev/nvidia-cap12, /dev/nvidia-cap13 and \n                           /dev/nvidia-cap14 and /dev/nvidia-cap15 would be created. Thus a MIG device is identified by the following format: \n                           \n                         \n                           As an example, having read access to the following paths would allow one to run workloads on the MIG device represented by\n                           \n                           <gpu0, gi0, ci0>:\n                           \n                         \n                           Note, that there is no access file representing a capability to run workloads on gpu0 (only on gi0 and ci0 that sit underneath\n                           gpu0)", "Installation Guides Programming Guides CUDA API References PTX Compiler API References Miscellaneous Tools White Papers Application Notes Compiler SDK Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA\u00ae CUDA\u00ae Toolkit provides a development environment for creating high performance GPU-accelerated\r\napplications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated\r\nembedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\r\nThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime\r\nlibrary to deploy your application. The Release Notes for the CUDA Toolkit. The list of CUDA features by release. The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system. This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems. This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems. This guide provides a detailed discussion of the CUDA programming model and programming interface. The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API. This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code. This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code. The CUDA math API. The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime. The cuDLA API. The API reference for libcu++, the CUDA C++ standard library. The NVIDIA\u00ae GPUDirect\u00ae Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology. The cuRAND library user guide. The cuSPARSE library user guide. NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API. The cuSOLVER library user guide. This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library. This document describes the demo applications shipped with the CUDA Demo Suite. This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2). The guide covers installation and running CUDA applications and containers in this environment. This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade. The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications. The CUDA debugger API. vGPUs that support CUDA. This is a reference document for nvcc, the CUDA compiler driver. The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware. CUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger", "User guide for Multi-Instance GPU on the NVIDIA\u00ae GPUs. This edition of the user guide describes the Multi-Instance GPU feature first introduced\n                        with the NVIDIA\u00ae Ampere architecture.\n                       \n                        The new Multi-Instance GPU (MIG) feature allows GPUs (starting with NVIDIA Ampere \n                        architecture) to be securely partitioned into up to seven \n                        separate GPU Instances for CUDA applications, providing multiple users with separate \n                        GPU resources for optimal GPU utilization. This feature is particularly beneficial for \n                        workloads that do not fully saturate the GPU's compute capacity and therefore users may want to \n                        run different workloads in parallel to maximize utilization. \n                        \n                      \n                        With MIG, each instance's processors have separate and isolated paths through the entire\n                        memory system - the on-chip crossbar ports, L2 cache banks, memory controllers, and DRAM\n                        address busses are all assigned uniquely to an individual instance. This ensures that an\n                        individual user's workload can run with predictable throughput and latency, with the same L2\n                        cache allocation and DRAM bandwidth, even if other tasks are thrashing their own caches or\n                        saturating their DRAM interfaces. MIG can partition available GPU compute resources (including \n                        streaming multiprocessors or SMs, and GPU engines such as copy engines or decoders), to provide \n                        a defined quality of service (QoS) with fault isolation for different clients such as VMs, \n                        containers or processes. MIG enables multiple GPU Instances to run in parallel on a single, physical \n                        NVIDIA Ampere GPU.\n                        \n                      \n                        With MIG, users will be able to see and schedule jobs on their new virtual GPU Instances as \n                        if they were physical GPUs.\n                        \n                      Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single GPU, while preserving the isolation guarantees\n                        \n                        that vGPU provides. For more information on GPU partitioning using vGPU and MIG, refer to the \n                        technical brief. GPUs with\n                        compute capability >= 8.\n                        \n                      Bare-metal, including containers and Kubernetes GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors \n                                 Under Linux guests on supported hypervisors, when MIG-supported GPUs are \n                                 in GPU pass-through, the same \n                                 workflows, tools \n                                 and profiles available \n                                 on bare-metal can be used.\n                                 \n                               \n                                 MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single \n                                 MIG-supported GPU, while preserving the isolation guarantees that vGPU provides.\n                                 To configure a GPU for use with vGPU VMs, refer to the \n                                 chapter in the vGPU Software User\n                                 Guide.\n                                 Refer also to the \n                                 technical brief for more information on GPU partitioning with vGPU.\n                           \n                         A streaming multiprocessor (SM) executes compute instructions on the GPU. Fault isolation Individually scheduled Distinct address space A GPU engine is what executes work on the GPU. The most commonly used engine is the \n                              Compute/Graphics engine that executes the compute instructions. Each engine can be scheduled independently and execute work for different \n                              GPU contexts.\n                            A GPU memory slice is the smallest fraction of the GPU's memory, including the \n                              corresponding memory controllers and cache. A GPU memory slice is roughly one eighth of the \n                              total GPU memory resources, including both capacity and bandwidth.\n                            A GPU SM slice is the smallest fraction of the SMs on the GPU.\n                            A GPU slice is the smallest fraction of the GPU that combines a single GPU memory slice \n                              and a single GPU SM slice.\n                            A GPU Instance (GI) is a combination of GPU slices and GPU engines (DMAs, NVDECs, etc. \n                              Anything within a GPU instance always shares all the GPU memory slices and other GPU engines, \n                              but it's SM slices can be further subdivided into compute instances (CI). A GPU instance provides \n                              memory QoS. Each GPU slice includes dedicated GPU memory resources which limit both the available \n                              capacity and bandwidth, and provide memory QoS. Each GPU memory slice gets 1/8 of the total GPU \n                              memory resources and each GPU SM slice gets 1/7 of the total number of SMs. \n                            A GPU instance can be subdivided into multiple compute instances. A Compute Instance (CI) contains \n                              a subset of the parent GPU instance's SM slices and other GPU engines (DMAs, NVDECs, etc. \n                              The CIs share memory and engines.\n                            \n                           Using the concepts introduced above, this section provides an overview of how the user can \n                           create various partitions on the GPU. \n                           \n                         \n                              Partitioning of the GPU happens using memory slices, so the A100-40GB GPU can be thought \n                              of having 8x5GB memory slices and 7 SM slices as shown in the diagram below. \n                              \n                            \n                              As explained above, then to create a GPU Instance (GI) requires combining some number of \n                              memory slices with some number of compute slices.5gb GI profile:\n                              \n                            \n                              The compute slices of a GPU Instance can be further subdivided into multiple \n                              Compute Instances (CI), with the CIs sharing the engines and memory of the parent GI, \n                              but each CI has dedicated SM resources.20gb example above, a CI may be created to consume only the first \n                              compute slice as shown below:\n                              \n                            \n                              In this case, 4 different CIs can be created by choosing any of the compute slices. Two compute \n                              slices can also be combined together to create a 2c.20gb profile:\n                              \n                            \n                              In this example, 3 compute slices can also be combined to create a 3c.\n                              \n                            \n                              The number of slices that a GI can be created with is not arbitrary. The NVIDIA driver \n                              APIs provide a number of \u201cGPU Instance Profiles\u201d and users can create GIs by specifying \n                              one of these profiles.\n                              \n                            \n                              On a given GPU, multiple GIs can be created from a mix and match of these profiles, so long \n                              as enough slices are available to satisfy the request.\n                                 \n                               The diagram below shows a pictorial representation of how to build all valid\n                              combinations of GPU instances. \n                            In this diagram, a valid combination can be built by starting with an instance\n                              profile on the left and combining it with other instance profiles as you move to the\n                              right, such that no two profiles overlap vertically.\n                            \n                              Note that the diagram represents the physical layout of where the GPU Instances will exist once they \n                              are instantiated on the GPU. As GPU Instances are created and destroyed at different locations, \n                              fragmentation can occur, and the physical position of one GPU Instance will play a role in which other \n                              GPU Instances can be instantiated next to it. CUDA already \n                           exposes multiple technologies for running work in parallel on the GPU and it is worthwhile \n                           showcasing how these technologies compare to MIG. Note that streams and MPS are part of the \n                           CUDA programming model and thus work when used with GPU Instances.\n                           \n                         \n                           CUDA Streams are a CUDA Programming model feature where, in a CUDA application, different \n                           work can be submitted to independent queues and be processed independently by the GPU. \n                           CUDA streams can only be used within a single process and don't offer much isolation - the \n                           address space is shared, the SMs are shared, the GPU memory bandwidth, caches and capacity \n                           are shared.\n                           \n                         \n                           MPS is the CUDA Multi-Process service. It allows co-operative multi process applications to share \n                           compute resources on the GPU. It's commonly used by MPI jobs that cooperate, but it has also been \n                           used for sharing the GPU resources among unrelated applications, while accepting the challenges that \n                           such a solution brings. MPS currently does not offer error isolation between clients and while \n                           streaming multiprocessors used by each MPS client can be optionally limited to a percentage of all SMs, \n                           the scheduling hardware is still shared. \n                           \n                         \n                           Lastly, MIG is the new form of concurrency offered by NVIDIA GPUs while addressing some of the \n                           limitations with the other CUDA technologies for running parallel work.\n                                    \n                                  Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors  MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single A100, while preserving the isolation guarantees\n                                 \n                                 that vGPU provides. For more information on GPU partitioning using vGPU and MIG, refer to the \n                                 technical brief. Note \n                                 that the setting is on a per-GPU basis. OpenGL, Vulkan etc.) No GPU to GPU P2P (either PCIe or NVLink) is supported CUDA applications treat a Compute Instance and its parent GPU Instance as a single CUDA device. \n                                 See this section on device enumeration by CUDA\n                               CUDA IPC across GPU instances is not supported. CUDA IPC across Compute instances is supported CUDA debugging (e. using cuda-gdb) and memory/race checking \n                                 (e. The only limitation is that the maximum number of clients (48) is \n                                 lowered proportionally to the Compute Instance size\n                                 \n                               GPUDirect RDMA is supported when used from GPU Instances\n                                 \n                               \n                        By default, a MIG device consists of a single \u201cGPU Instance\u201d and a single \u201cCompute Instance\u201d. \n                        The table below highlights a naming convention to refer to a MIG device by its GPU Instance's \n                        compute slice count and its total memory in GB (rather than just its memory slice count).\n                        \n                      \n                        When only a single CI is created (that consumes the entire compute capacity of the GI), \n                        then the CI sizing is implied in the device name.\n                           \n                         \n                        Each GI can be further sub-divided into multiple CIs as required by users depending on their workloads.20gb device into a set of sub-devices with different Compute Instance slice counts.\n                        \n                      \n                           GPU Instances (GIs) and Compute Instances (CIs) are enumerated in the new /proc filesystem layout for MIG\n                           \n                         \n                           The corresponding device nodes (in mig-minors) are created under /dev/nvidia-caps. In other words, \n                           regardless of how many MIG devices are created (or made available to a container), a single CUDA process \n                           can only enumerate a single MIG device. \n                           \n                         CUDA can only enumerate a single compute instance CUDA will not enumerate non-MIG GPU if any compute instance is enumerated on any other GPU \n                           Note that these constraints may be relaxed in future NVIDIA driver releases for MIG.01+), the example below shows \n                              how MIG devices are assigned GPU UUIDs in an 8-GPU system with each GPU configured differently. The GPUs can be selected using comma separated \n                           GPU indexes, PCI Bus Ids or UUIDs. monitoring agents) that use the GPU, then you may not be able to initiate\n                              a GPU reset. \n                           The profiles are the sizes and capabilities of the GPU instances that can be created by the user. The syntax of the placement is \n                           {<index>}:<GPU Slice Count> and shows the placement of the instances on the GPU.\n                           The placement index shown indicates how the profiles are mapped on the GPU as shown in the \n                           supported profiles tables. \n                           \n                         \n                           Once the GPU instances are created, one needs to create the corresponding Compute Instances (CI). \n                           \n                         Without creating GPU instances (and corresponding compute instances), CUDA workloads cannot be run on the GPU. \n                              \n                            \n                           The following example shows how the user can create GPU instances (and corresponding compute instances). In this example,\n                           the user can create \n                           two GPU instances (of type 3g.20gb), with each GPU instance having half of the available compute and memory capacity. In this \n                           example, we purposefully use profile ID and short profile name to showcase how either option can be used:\n                           \n                         \n                           Now list the available GPU instances:\n                           \n                         \n                           Now verify that the GIs and corresponding CIs are created:\n                           \n                         As described in the section on \n                                 Partitioning, the NVIDIA driver APIs provide a number of available GPU Instance profiles that can be chosen by the user. After the instances are created, the placement of the profiles can be observed:\n                              \n                            Example 2: Creation of a 3-2-1-1 geometry. \n                              \n                            Example 3: Creation of a 2-1-1-1-1-1 geometry:\n                              \n                            \n                              The following example shows how two CUDA applications can be run in parallel on two different GPU instances. In this example,\n                              \n                              the BlackScholes CUDA sample is run simultaneously on the two GIs created on the A100.\n                              \n                            \n                              Now verify the two CUDA applications are running on two separate GPU instances:\n                              \n                            NVML (and nvidia-smi) does not support attribution of utilization metrics to MIG devices. From the previous example, \n                                 the utilization is displayed as N/A when running CUDA programs:\n                                 \n                               \n                                 For monitoring MIG devices on MIG capable GPUs such as the A100, including attribution of GPU metrics \n                                 (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v2. \n                                 See the Profiling Metrics \n                                 section in the DCGM User Guide for more details on getting started.\n                                 \n                               \n                              As explained earlier in this document, a further level of concurrency can be achieved by using Compute Instances (CIs). \n                              The following example shows how 3 CUDA processes (BlackScholes CUDA sample) can be run on the same GI.\n                              \n                            \n                              Create 3 CIs, each of type 1c compute capacity (profile ID 0) on the first GI. \n                              \n                            \n                              And the GIs and CIs created on the A100 are now enumerated by the driver:\n                              \n                            \n                              Now, three BlackScholes applications can be created and run in parallel:\n                              \n                            \n                              And seen using nvidia-smi as running processes on the three CIs:\n                              \n                            \n                           Once the GPU is in MIG mode, GIs and CIs can be configured dynamically. \n                           The following example shows how the CIs and GIs created in the previous examples can be destroyed.\n                           \n                         \n                           It can be verified that the CI devices have now been torn down on the GPU:\n                           \n                         \n                           Now the GIs have to be deleted: \n                           \n                         \n                           For monitoring MIG devices on including attribution of GPU metrics \n                           (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v3 or later. \n                           See the Profiling Metrics \n                           section in the DCGM User Guide for more details on getting started. From the previous example, \n                              the utilization is displayed as N/A when running CUDA programs:\n                              \n                            \n                           As described in the section on CUDA concurrency mechanisms, \n                           CUDA Multi-Process \n                              Service (MPS) enables co-operative multi-process CUDA applications to be processed \n                           concurrently on the GPU.\n                                       \n                                     \n                                       Setup the CUDA_MPS_PIPE_DIRECTORY variable to point to unique directories \n                                       so that the multiple MPS servers and clients can communicate with each other using named pipes and \n                                       Unix domain sockets.\n                                       \n                                     \n                                    The MPS documentation recommends setting up EXCLUSIVE_PROCESS mode to ensure that a single \n                                    MPS server is using the GPU. For this example,\n                              we configure the GPU into a 3g.2gb geometry: \n                              \n                            \n                              In this step, we start an MPS control daemon (with admin privileges) and ensure we use a different socket for each daemon:\n                              \n                            \n                              Now we can launch the application by specifying the desired MIG device using CUDA_VISIBLE_DEVICES:\n                              \n                            \n                              We now provide a script below where we attempt to run the BlackScholes from before on the two MIG devices \n                              created on the GPU:\n                              \n                            \n                              When running this script, we can observe the two MPS servers on each MIG device and the corresponding CUDA program started\n                              as an \n                              MPS client when using nvidia-smi:\n                              \n                             \n                              NVIDIA Container Toolkit has been enhanced to provide support for MIG devices, allowing users to run \n                           GPU containers with runtimes such as Docker.\n                              \n                            \n                              A more complex example is to run a TensorFlow container to do a training run using GPUs on the MNIST dataset.\n                           \n                         Slurm is a \n                           workload manager that is widely used at high performance computing centers such as government labs, universities. \n                        Each physical GPU is represented by its own device node - e. \n                        This is shown below for a 2-GPU system. the A100 GPU).\n                           \n                         \n                           For example, granting a container the mig-config capability implies that we should also grant it capabilities to access all possible gis and cis that could be created \n                           for any GPU on the system.\n                           \n                         \n                           For example, the mig-config capability (which allows a user to create and destroy MIG devices) is represented as follows:\n                           \n                         \n                           Likewise, the capabilities required to run workloads on a MIG device once it has been created are represented as follows \n                           (namely as access to the GPU Instance and Compute Instance that comprise the MIG device):\n                           \n                         \n                           And the corresponding file system layout is shown below with read permissions:\n                           \n                         \n                           For a CUDA process to be able to run on top of MIG, it needs access to the Compute Instance \n                           capability and its parent GPU Instance. \n                           This is because the traditional mechanism of using cgroups to control access to top level GPU devices (and any required meta\n                           devices) is still required", "  \n        \n         \n          The APIs and descriptions are subject to change without notice.2 (GDS version 1.7. peer to peer transfer using GPUDirect\u2122 is supported to and from device memory on supported file system and hardware configurations.  Its use is optional.  Refer to the GPUDirect Best Practices Guide for more information. The best practice is to always call these APIs in the application cleanup paths.  If this option is false, all writes to WekaFS will use compatibility mode.  \n\"rdma_dynamic_routing\": false, \n\"rdma_dynamic_routing_order\": [ \" \"SYS_MEM\" ]  For wekaFS or IBM Spectrum Scale mounts: If there are no rdma_dev_addr_list specified, or failure to register MR with ib device. properties. properties.   \n             Data path errors are captured via standard error codes by using errno. The APIs will return -1 on error. \n         \n \n          Refer to sample sample_003 for usage.  \n        \n          These operations can be submitted on different files, different locations in the same file, or a mix. The bytes transacted field is valid only when the status indicates a successful completion. \n          \n           Refer to samples sample_019, sample_020, sample_021, and sample_022 for usage.   See the GPUDirect Storage Overview Guide for a high-level analysis of the set of functions and their relation to each other. We anticipate adding additional return codes for some of these functions.log file for more information.json) for more information. The structure is self-describing, and its fields are consistent with the major and minor API version parameters.poll_modeand properties.2 (GDS version 1.7.x) supports non O_DIRECT open flags as well as O_DIRECT.ko installed.  Valid pointer to the OS-neutral file descriptor supplied by the user carrying details regarding the file to be opened such as fd for Linux-based files.log file for valid inputs.  \n          \n           GDS functionality modified the standard file system metadata in SysMem. However, GDS functionality does not take any special responsibility for writing that metadata back to permanent storage. If the file is opened with an O_SYNC flag, the metadata will be written to the disk before the call is complete. Reserved for future use, must be 0.  The following are valid values:    \n           Using the flag \u20180XF\u2019 will perform best as the workflow can be optimized during submission time.  Reserved for future use.  Reserved for future use. Should be set to 0.  The bytes transacted field is valid only when the status indicates a completion. The min_nr should be greater than or equal to zero and less than or equal to *nr.   Returns\n The success here refers to the completion of the API./gds_helloworld\n//\n//\n#include <fcntl.h>\n#include <errno.h>\n#include <unistd.  Exiting.2\n        // (gds 1.7.handle.fd = fd;\n        cf_descr. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\n   Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. No contractual obligations are formed either directly or indirectly by this document. It is customer\u2019s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\n \n \n  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. Other company and product names may be trademarks of the respective companies with which they are associated", "  Because the functionality is part of the CUDA Driver C API, the APIs use the cuFile prefix and camel case motif of the CUDA Driver.  \n        \n \n          Starting from CUDA toolkit 12. For these errors, the output parameters of the APIs are left in an undefined state, and there are no other side effects.  \n          cuFileDriverOpen explicitly causes driver initialization.  If this API is not called, the buffer and resources are implicitly freed when the driver is closed using cuFileDriverClose.   cuFileDriverClose explicitly frees driver resources.  If this API is not called, the driver resources are implicitly freed when dlclose() is performed on the library handle or when the process is terminated.  Note: This option will force posix mode even if \u201callow_compat_mode\u201d is set to \u201cfalse\u201d.err == CU_FILE_CUDA_DRIVER_ERROR)\n \n#define CU_FILE_CUDA_ERR(status) ((status).use_poll_mode*/\n      CU_FILE_ALLOW_COMPAT_MODE = 1 /*!< allow COMPATIBILITY mode.\n               #define CUFILEOP_BASE_ERR 5000 \n    A CUDA Driver API error.  This error indicates a CUDA driver-api error. If this is set, a CUDA-specific error code is set in the cu_err field for cuFileError.allow_compat_mode is set to true.allow_compat_mode in the /etc/cufile.json file should be set to false. \n      \n \n  \n       This section provides information about the cuFileDriver API functional specification.  \n           Description\n If the JSON configuration file does not exist, the API loads the default library settings. To modify this default config file, administrative privileges are needed.  CUfileError_t cuFileDriverGetProperties(cuFileDrvProps_t *props); \n  Parameters\n  props\n  Returns\n  Description\n  This API is used to get current GDS properties and nvidia-fs driver properties and functionality, such as support for SCSI, NVMe, and NVMe-OF.  \n           This API is used to get the current nvidia-fs drivers-specific properties such as the following:  \n          Otherwise, the compatible mode is disabled.  \n           This API overrides the default value that may be set through the JSON configuration file using the config keys properties.  \n           This API overrides the default value that might be set through the JSON configuration file by using the properties.  \n           See cuFileDriverGetProperties for more information. SeecuFileDriverGetProperties for more information.   Returns\n  None \n          \n           \n            This API only logs an ERROR level message in the cufile. If NULL, make this operation in the default CUDA stream. If NULL, make this operation in the default CUDA stream"]